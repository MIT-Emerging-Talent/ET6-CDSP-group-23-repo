{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preparation for Transfer Analysis\n",
        "\n",
        "This notebook outlines the complete process of cleaning raw football player statistics, handling missing values, standardizing data, and preparing datasets for transfer analysis. The goal is to make the data easily understandable and replicable.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Initial Setup and Data Loading\n",
        "\n",
        "This section imports necessary libraries and defines utility functions for the initial cleaning steps, such as dropping columns with percentage values, extracting player names from URLs, and handling various data types and missing values.\n",
        "\n",
        "### 1.1 Import Libraries\n",
        "\n",
        "We start by importing all the required libraries for data manipulation and regular expressions."
      ],
      "metadata": {
        "id": "gHkY5J6L8RmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "S3-vH_c7XHwr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Define Data Cleaning Functions\n",
        "\n",
        "These functions encapsulate specific cleaning logic, making the main processing loop cleaner and more readable.\n",
        "\n",
        "#### `drop_columns_with_percent(df)`\n",
        "\n",
        "This function identifies and removes columns that contain percentage signs, as these often represent calculated metrics that might not be suitable for direct comparison or aggregation."
      ],
      "metadata": {
        "id": "kmBbMiY78bD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_columns_with_percent(df):\n",
        "    \"\"\"\n",
        "    Drops columns from a pandas DataFrame if any of their values (as strings)\n",
        "    contain a '%' sign. This is useful for removing calculated metrics that\n",
        "    might not be directly comparable or are redundant.\n",
        "    \"\"\"\n",
        "    columns_to_drop = [col for col in df.columns if df[col].astype(str).str.contains('%', na=False).any()]\n",
        "    if columns_to_drop:\n",
        "        print(f\"Dropping columns containing '%': {', '.join(columns_to_drop)}\")\n",
        "        return df.drop(columns=columns_to_drop)\n",
        "    else:\n",
        "        print(\"No columns with '%' found to drop.\")\n",
        "        return df"
      ],
      "metadata": {
        "id": "_XOulOHXC055"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `extract_and_format_name(url)`\n",
        "\n",
        "This helper function extracts a player's name from a given URL string and formats it into a more readable title-case format."
      ],
      "metadata": {
        "id": "n02XR5gh8iLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_format_name(url):\n",
        "    \"\"\"\n",
        "    Extracts and formats a player's name from a URL.\n",
        "    Example: '.../some-player-name' becomes 'Some Player Name'.\n",
        "    Returns None if the URL is NaN or cannot be parsed.\n",
        "    \"\"\"\n",
        "    if pd.isna(url):\n",
        "        return None\n",
        "    match = re.search(r'/([^/]+)$', url) # Regex to find string after the last '/'\n",
        "    if match:\n",
        "        # Capitalize each word in the extracted name (e.g., 'some-player-name' -> 'Some Player Name')\n",
        "        return ' '.join(word.capitalize() for word in match.group(1).replace('_', '-').split('-'))\n",
        "    return None"
      ],
      "metadata": {
        "id": "7KdTC67d8kE8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `process_url_column(df)`\n",
        "\n",
        "This function applies the `extract_and_format_name` function to the 'URL' column and renames it to 'Player Name'. This is crucial for uniquely identifying players across different datasets."
      ],
      "metadata": {
        "id": "ewBM5eaY8oN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_url_column(df):\n",
        "    \"\"\"\n",
        "    Processes the 'URL' column to extract player names and renames the column to 'Player Name'.\n",
        "    This standardizes player identification.\n",
        "    \"\"\"\n",
        "    if 'URL' in df.columns:\n",
        "        df = df.copy()\n",
        "        df['URL'] = df['URL'].apply(extract_and_format_name)\n",
        "        df = df.rename(columns={'URL': 'Player Name'})\n",
        "        print(\"Processed 'URL' column to 'Player Name'.\")\n",
        "    else:\n",
        "        print(\"'URL' column not found, skipping URL processing.\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "aXcXqsGx8rF7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `clean_and_impute(df)`\n",
        "\n",
        "This function handles data type conversions and imputes missing values. It specifically addresses common issues like commas in numeric strings and fills specific columns with zeros, while imputing others with the median."
      ],
      "metadata": {
        "id": "Zgzff3LC8tVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_impute(df):\n",
        "    \"\"\"\n",
        "    Performs data type conversion and imputation on the DataFrame.\n",
        "    Specifically handles 'Minutes Played' and 'Possession - Touches' by converting them to numeric,\n",
        "    imputes 'Defending - Penalties conceded' and 'Possession - Penalties awarded' with 0,\n",
        "    and fills other numerical missing values with their respective column medians.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert columns that might contain commas as thousands separators to numeric\n",
        "    columns_to_clean_numeric = ['Minutes Played', 'Possession - Touches']\n",
        "\n",
        "    for col in columns_to_clean_numeric:\n",
        "        if col in df.columns and df[col].dtype == 'object':\n",
        "            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce')\n",
        "            print(f\"Converted '{col}' to numeric.\")\n",
        "        elif col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            print(f\"'{col}' is already numeric.\")\n",
        "        else:\n",
        "            print(f\"'{col}' not found or not an object type, skipping numeric conversion.\")\n",
        "\n",
        "\n",
        "    # Impute specific columns with 0, as these often represent counts that were zero if not recorded.\n",
        "    columns_to_impute_zero = ['Defending - Penalties conceded', 'Possession - Penalties awarded']\n",
        "    for col in columns_to_impute_zero:\n",
        "        if col in df.columns and df[col].isnull().sum() > 0:\n",
        "            nan_count = df[col].isnull().sum()\n",
        "            df[col].fillna(0, inplace=True)\n",
        "            print(f\"Imputed {nan_count} missing values in '{col}' with 0.\")\n",
        "        elif col in df.columns:\n",
        "            print(f\"'{col}' has no missing values.\")\n",
        "        else:\n",
        "            print(f\"'{col}' not found, skipping zero imputation.\")\n",
        "\n",
        "    # Impute remaining numerical columns with their median to preserve distribution shape and reduce outlier impact.\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[col]) and df[col].isnull().sum() > 0:\n",
        "            nan_count = df[col].isnull().sum()\n",
        "            median_val = df[col].median()\n",
        "            df[col].fillna(median_val, inplace=True)\n",
        "            print(f\"Imputed {nan_count} missing values in '{col}' with median ({median_val}).\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "3YQRJbdD8vdX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Processing Raw Datasets\n",
        "\n",
        "This section iterates through the raw CSV files, applies the defined cleaning functions, and ensures all datasets have a consistent set of columns before saving them.\n",
        "\n",
        "### 2.1 Define File Paths\n",
        "\n",
        "List all the raw CSV files that need to be processed."
      ],
      "metadata": {
        "id": "KZazmPAs84OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of CSV files to process\n",
        "csv_file_paths = [\n",
        "    '2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv',\n",
        "    '2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv',\n",
        "    '2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv',\n",
        "    '2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv',\n",
        "    '2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv',\n",
        "]\n",
        "\n",
        "processed_dfs = []"
      ],
      "metadata": {
        "id": "C5aaYi8B85M2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Loop Through Files and Apply Cleaning\n",
        "\n",
        "Each file is read, cleaned using the defined functions, and then added to a list of processed DataFrames. Error handling is included to catch missing files or other processing issues."
      ],
      "metadata": {
        "id": "Ru_EhAOd898I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file_path in csv_file_paths:\n",
        "    print(f\"\\n{'='*10} Processing file: {file_path} {'='*10}\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "        df_cleaned = drop_columns_with_percent(df)\n",
        "        df_cleaned = process_url_column(df_cleaned)\n",
        "        df_cleaned = clean_and_impute(df_cleaned)\n",
        "\n",
        "        processed_dfs.append(df_cleaned)\n",
        "        print(f\"Successfully processed {file_path}. Cleaned shape: {df_cleaned.shape}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found. Please ensure it's in the correct directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing '{file_path}': {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYWrJLkY9AMY",
        "outputId": "9cc5d7ef-ddf8-4da9-d57e-0550226097b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Processing file: 2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv ==========\n",
            "Original shape: (84, 43)\n",
            "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
            "'URL' column not found, skipping URL processing.\n",
            "Converted 'Minutes Played' to numeric.\n",
            "'Possession - Touches' is already numeric.\n",
            "Imputed 64 missing values in 'Defending - Penalties conceded' with 0.\n",
            "Imputed 72 missing values in 'Possession - Penalties awarded' with 0.\n",
            "Imputed 7 missing values in 'Defending - Aerial duels won' with median (1.36).\n",
            "Imputed 9 missing values in 'Defending - Blocked' with median (0.29).\n",
            "Imputed 4 missing values in 'Defending - Dribbled past' with median (0.815).\n",
            "Imputed 3 missing values in 'Defending - Duels won' with median (5.29).\n",
            "Imputed 4 missing values in 'Defending - Fouls committed' with median (1.085).\n",
            "Imputed 5 missing values in 'Defending - Interceptions' with median (1.13).\n",
            "Imputed 12 missing values in 'Defending - Possession won final 3rd' with median (0.36).\n",
            "Imputed 3 missing values in 'Defending - Recoveries' with median (5.16).\n",
            "Imputed 3 missing values in 'Defending - Tackles won' with median (0.97).\n",
            "Imputed 3 missing values in 'Discipline - Red cards' with median (0.0).\n",
            "Imputed 3 missing values in 'Discipline - Yellow cards' with median (0.15).\n",
            "Imputed 3 missing values in 'Minutes Played' with median (2014.0).\n",
            "Imputed 5 missing values in 'Passing - Accurate long balls' with median (2.32).\n",
            "Imputed 3 missing values in 'Passing - Assists' with median (0.07).\n",
            "Imputed 5 missing values in 'Passing - Chances created' with median (0.96).\n",
            "Imputed 17 missing values in 'Passing - Successful crosses' with median (0.43).\n",
            "Imputed 3 missing values in 'Passing - Successful passes' with median (30.45).\n",
            "Imputed 3 missing values in 'Possession - Dispossessed' with median (1.03).\n",
            "Imputed 4 missing values in 'Possession - Fouls won' with median (1.1349999999999998).\n",
            "Imputed 8 missing values in 'Possession - Successful dribbles' with median (1.095).\n",
            "Imputed 3 missing values in 'Possession - Touches' with median (58.18).\n",
            "Imputed 4 missing values in 'Possession - Touches in opposition box' with median (1.395).\n",
            "Imputed 3 missing values in 'ShootingRank - Goals' with median (0.07).\n",
            "Imputed 3 missing values in 'ShootingRank - Shots' with median (1.0).\n",
            "Imputed 7 missing values in 'ShootingRank - Shots on target' with median (0.31).\n",
            "Successfully processed 2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv. Cleaned shape: (84, 30)\n",
            "\n",
            "========== Processing file: 2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv ==========\n",
            "Original shape: (115, 44)\n",
            "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
            "Processed 'URL' column to 'Player Name'.\n",
            "Converted 'Minutes Played' to numeric.\n",
            "Converted 'Possession - Touches' to numeric.\n",
            "Imputed 100 missing values in 'Defending - Penalties conceded' with 0.\n",
            "Imputed 87 missing values in 'Possession - Penalties awarded' with 0.\n",
            "Imputed 8 missing values in 'Defending - Aerial duels won' with median (1.34).\n",
            "Imputed 13 missing values in 'Defending - Blocked' with median (0.455).\n",
            "Imputed 6 missing values in 'Defending - Dribbled past' with median (0.89).\n",
            "Imputed 5 missing values in 'Defending - Duels won' with median (5.385).\n",
            "Imputed 7 missing values in 'Defending - Fouls committed' with median (1.165).\n",
            "Imputed 8 missing values in 'Defending - Interceptions' with median (0.87).\n",
            "Imputed 11 missing values in 'Defending - Possession won final 3rd' with median (0.55).\n",
            "Imputed 5 missing values in 'Defending - Recoveries' with median (4.99).\n",
            "Imputed 5 missing values in 'Defending - Tackles won' with median (0.865).\n",
            "Imputed 5 missing values in 'Discipline - Red cards' with median (0.0).\n",
            "Imputed 5 missing values in 'Discipline - Yellow cards' with median (0.12).\n",
            "Imputed 5 missing values in 'Minutes Played' with median (2078.5).\n",
            "Imputed 8 missing values in 'Passing - Accurate long balls' with median (1.23).\n",
            "Imputed 5 missing values in 'Passing - Assists' with median (0.09).\n",
            "Imputed 6 missing values in 'Passing - Chances created' with median (1.0).\n",
            "Imputed 24 missing values in 'Passing - Successful crosses' with median (0.37).\n",
            "Imputed 5 missing values in 'Passing - Successful passes' with median (25.7).\n",
            "Imputed 5 missing values in 'Possession - Dispossessed' with median (1.39).\n",
            "Imputed 7 missing values in 'Possession - Fouls won' with median (1.165).\n",
            "Imputed 7 missing values in 'Possession - Successful dribbles' with median (1.005).\n",
            "Imputed 5 missing values in 'Possession - Touches' with median (52.65).\n",
            "Imputed 7 missing values in 'Possession - Touches in opposition box' with median (3.185).\n",
            "Imputed 5 missing values in 'ShootingRank - Goals' with median (0.125).\n",
            "Imputed 89 missing values in 'ShootingRank - Penalty goals' with median (0.07).\n",
            "Imputed 5 missing values in 'ShootingRank - Shots' with median (1.725).\n",
            "Imputed 15 missing values in 'ShootingRank - Shots on target' with median (0.655).\n",
            "Successfully processed 2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv. Cleaned shape: (115, 31)\n",
            "\n",
            "========== Processing file: 2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv ==========\n",
            "Original shape: (106, 48)\n",
            "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
            "'URL' column not found, skipping URL processing.\n",
            "Converted 'Minutes Played' to numeric.\n",
            "'Possession - Touches' is already numeric.\n",
            "Imputed 76 missing values in 'Defending - Penalties conceded' with 0.\n",
            "Imputed 79 missing values in 'Possession - Penalties awarded' with 0.\n",
            "Imputed 1 missing values in 'Average Rating' with median (6.88).\n",
            "Imputed 17 missing values in 'Defending - Blocked' with median (0.2).\n",
            "Imputed 3 missing values in 'Defending - Dribbled past' with median (0.66).\n",
            "Imputed 1 missing values in 'Defending - Fouls committed' with median (1.01).\n",
            "Imputed 1 missing values in 'Defending - Interceptions' with median (0.96).\n",
            "Imputed 16 missing values in 'Defending - Possession won final 3rd' with median (0.37).\n",
            "Imputed 1 missing values in 'Defending - Tackles won' with median (0.92).\n",
            "Imputed 1 missing values in 'Passing - Accurate long balls' with median (1.64).\n",
            "Imputed 6 missing values in 'Passing - Chances created' with median (0.74).\n",
            "Imputed 49 missing values in 'Passing - Expected assists (xA)' with median (0.04).\n",
            "Imputed 27 missing values in 'Passing - Successful crosses' with median (0.31).\n",
            "Imputed 3 missing values in 'Possession - Fouls won' with median (0.9).\n",
            "Imputed 7 missing values in 'Possession - Successful dribbles' with median (0.89).\n",
            "Imputed 2 missing values in 'Possession - Touches in opposition box' with median (1.46).\n",
            "Imputed 51 missing values in 'ShootingRank - Expected goals (xG)' with median (0.04).\n",
            "Imputed 93 missing values in 'ShootingRank - Penalty goals' with median (0.1).\n",
            "Imputed 11 missing values in 'ShootingRank - Shots on target' with median (0.31).\n",
            "Imputed 59 missing values in 'ShootingRank - xG on target (xGOT)' with median (0.05).\n",
            "Imputed 106 missing values in 'Unnamed: 47' with median (nan).\n",
            "Successfully processed 2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv. Cleaned shape: (106, 35)\n",
            "\n",
            "========== Processing file: 2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv ==========\n",
            "Original shape: (76, 47)\n",
            "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
            "Processed 'URL' column to 'Player Name'.\n",
            "Converted 'Minutes Played' to numeric.\n",
            "'Possession - Touches' is already numeric.\n",
            "Imputed 64 missing values in 'Defending - Penalties conceded' with 0.\n",
            "Imputed 63 missing values in 'Possession - Penalties awarded' with 0.\n",
            "Imputed 2 missing values in 'Average Rating' with median (6.86).\n",
            "Imputed 9 missing values in 'Defending - Blocked' with median (0.43).\n",
            "Imputed 5 missing values in 'Defending - Dribbled past' with median (0.76).\n",
            "Imputed 7 missing values in 'Defending - Interceptions' with median (0.82).\n",
            "Imputed 4 missing values in 'Defending - Possession won final 3rd' with median (0.565).\n",
            "Imputed 1 missing values in 'Defending - Tackles won' with median (0.99).\n",
            "Imputed 3 missing values in 'Passing - Accurate long balls' with median (1.36).\n",
            "Imputed 1 missing values in 'Passing - Chances created' with median (1.02).\n",
            "Imputed 18 missing values in 'Passing - Expected assists (xA)' with median (0.08).\n",
            "Imputed 15 missing values in 'Passing - Successful crosses' with median (0.21).\n",
            "Imputed 2 missing values in 'Possession - Fouls won' with median (1.25).\n",
            "Imputed 19 missing values in 'ShootingRank - Expected goals (xG)' with median (0.11).\n",
            "Imputed 19 missing values in 'ShootingRank - Non-penalty xG' with median (0.11).\n",
            "Imputed 64 missing values in 'ShootingRank - Penalty goals' with median (0.065).\n",
            "Imputed 8 missing values in 'ShootingRank - Shots on target' with median (0.645).\n",
            "Successfully processed 2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv. Cleaned shape: (76, 34)\n",
            "\n",
            "========== Processing file: 2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-24-1054012734.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-24-1054012734.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (167, 48)\n",
            "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, ShootingRank - xG on target (xGOT), Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
            "Processed 'URL' column to 'Player Name'.\n",
            "Converted 'Minutes Played' to numeric.\n",
            "'Possession - Touches' is already numeric.\n",
            "Imputed 144 missing values in 'Defending - Penalties conceded' with 0.\n",
            "Imputed 129 missing values in 'Possession - Penalties awarded' with 0.\n",
            "Imputed 3 missing values in 'Defending - Aerial duels won' with median (1.35).\n",
            "Imputed 17 missing values in 'Defending - Blocked' with median (0.365).\n",
            "Imputed 6 missing values in 'Defending - Dribbled past' with median (0.79).\n",
            "Imputed 5 missing values in 'Defending - Fouls committed' with median (1.08).\n",
            "Imputed 8 missing values in 'Defending - Interceptions' with median (0.77).\n",
            "Imputed 12 missing values in 'Defending - Possession won final 3rd' with median (0.58).\n",
            "Imputed 3 missing values in 'Defending - Tackles won' with median (0.98).\n",
            "Imputed 7 missing values in 'Passing - Accurate long balls' with median (1.245).\n",
            "Imputed 5 missing values in 'Passing - Chances created' with median (0.995).\n",
            "Imputed 6 missing values in 'Passing - Expected assists (xA)' with median (0.09).\n",
            "Imputed 45 missing values in 'Passing - Successful crosses' with median (0.355).\n",
            "Imputed 3 missing values in 'Possession - Fouls won' with median (1.2650000000000001).\n",
            "Imputed 6 missing values in 'Possession - Successful dribbles' with median (1.2).\n",
            "Imputed 1 missing values in 'Possession - Touches in opposition box' with median (2.615).\n",
            "Imputed 10 missing values in 'ShootingRank - Expected goals (xG)' with median (0.13).\n",
            "Imputed 10 missing values in 'ShootingRank - Non-penalty xG' with median (0.12).\n",
            "Imputed 114 missing values in 'ShootingRank - Penalty goals' with median (1.02).\n",
            "Imputed 4 missing values in 'ShootingRank - Shots' with median (1.04).\n",
            "Imputed 18 missing values in 'ShootingRank - Shots on target' with median (0.36).\n",
            "Successfully processed 2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv. Cleaned shape: (167, 34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Identify Common Columns and Save Cleaned Data\n",
        "\n",
        "After all files are individually cleaned, this step identifies the common columns across all processed DataFrames. This is crucial for creating a unified dataset where players' statistics can be directly compared. Each cleaned DataFrame is then filtered to include only these common columns and saved to a new CSV file."
      ],
      "metadata": {
        "id": "XQPunNLZ9C_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if processed_dfs:\n",
        "    # Find the common columns across all processed DataFrames\n",
        "    # Using `reduce` with `intersection` ensures we get columns present in All dataframes.\n",
        "    common_columns = list(reduce(lambda left, right: left.intersection(right), [df.columns for df in processed_dfs]))\n",
        "    print(f\"\\nFound {len(common_columns)} common columns across all datasets.\")\n",
        "    print(\"Common columns found:\")\n",
        "    print(common_columns)\n",
        "\n",
        "    # Filter each DataFrame to keep only the common columns and save\n",
        "    for i, df in enumerate(processed_dfs):\n",
        "        file_path = csv_file_paths[i]\n",
        "        df_final = df[common_columns].copy()\n",
        "\n",
        "        # Verify no missing values remain\n",
        "        missing_after_all_steps = df_final.isnull().sum()\n",
        "        if missing_after_all_steps.sum() == 0:\n",
        "            print(f\"\\n✅ All missing values handled for {file_path}.\")\n",
        "        else:\n",
        "            print(f\"\\n⚠️ Warning: Missing values still remain in {file_path}:\")\n",
        "            print(missing_after_all_steps[missing_after_all_steps > 0])\n",
        "\n",
        "        # Define output file path\n",
        "        base_name = os.path.splitext(file_path)[0]\n",
        "        output_file_path = f\"{base_name}_cleaned_common.csv\"\n",
        "        df_final.to_csv(output_file_path, index=False)\n",
        "        print(f\"Final DataFrame for {file_path} saved to: {output_file_path}\")\n",
        "else:\n",
        "    print(\"\\nNo dataframes were processed. Please check file paths and previous steps.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv9EqUc09Fyf",
        "outputId": "cabfe4be-1700-41ae-a4af-d6ce487f1c88"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 30 common columns across all datasets.\n",
            "Common columns found:\n",
            "['Player Name', 'Season', 'Average Rating', 'Defending - Aerial duels won', 'Defending - Blocked', 'Defending - Dribbled past', 'Defending - Duels won', 'Defending - Fouls committed', 'Defending - Interceptions', 'Defending - Penalties conceded', 'Defending - Possession won final 3rd', 'Defending - Recoveries', 'Defending - Tackles won', 'Discipline - Red cards', 'Discipline - Yellow cards', 'Minutes Played', 'Passing - Accurate long balls', 'Passing - Assists', 'Passing - Chances created', 'Passing - Successful crosses', 'Passing - Successful passes', 'Possession - Dispossessed', 'Possession - Fouls won', 'Possession - Penalties awarded', 'Possession - Successful dribbles', 'Possession - Touches', 'Possession - Touches in opposition box', 'ShootingRank - Goals', 'ShootingRank - Shots', 'ShootingRank - Shots on target']\n",
            "\n",
            "✅ All missing values handled for 2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv.\n",
            "Final DataFrame for 2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv saved to: 2018-19_Transfers_2016-17_to_2019-20_Stats.raw_cleaned_common.csv\n",
            "\n",
            "✅ All missing values handled for 2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv.\n",
            "Final DataFrame for 2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv saved to: 2019-20_Transfers_2017-18_to_2020-21_Stats.raw_cleaned_common.csv\n",
            "\n",
            "✅ All missing values handled for 2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv.\n",
            "Final DataFrame for 2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv saved to: 2020-21_Transfers_2018-19_to_2021-22_Stats.raw_cleaned_common.csv\n",
            "\n",
            "✅ All missing values handled for 2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv.\n",
            "Final DataFrame for 2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv saved to: 2021-22_Transfers_2019-20_to_2022-23_Stats.raw_cleaned_common.csv\n",
            "\n",
            "✅ All missing values handled for 2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv.\n",
            "Final DataFrame for 2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv saved to: 2022-23_Transfers_2020-21_to_2023-24_Stats.raw_cleaned_common.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Merging Datasets into Pre and Post Transfer Aggregations\n",
        "\n",
        "This section focuses on combining the cleaned individual season datasets into two main datasets: one representing player statistics *before* a transfer season and another representing statistics *after* a transfer season. This is crucial for analyzing the impact of transfers.\n",
        "\n",
        "### 3.1 Define file paths for cleaned data\n",
        "\n",
        "List the paths to the newly created cleaned and common column CSV files"
      ],
      "metadata": {
        "id": "c6QRtYpiJs5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_cleaned_files = [\n",
        "    '2018-19_Transfers_2016-17_to_2019-20_Stats.raw_cleaned_common.csv',\n",
        "    '2019-20_Transfers_2017-18_to_2020-21_Stats.raw_cleaned_common.csv',\n",
        "    '2020-21_Transfers_2018-19_to_2021-22_Stats.raw_cleaned_common.csv',\n",
        "    '2021-22_Transfers_2019-20_to_2022-23_Stats.raw_cleaned_common.csv',\n",
        "    '2022-23_Transfers_2020-21_to_2023-24_Stats.raw_cleaned_common.csv',\n",
        "]\n",
        "\n",
        "# Initialize empty lists to collect all pre and post transfer dataframes\n",
        "pre_transfer_data = []\n",
        "post_transfer_data = []"
      ],
      "metadata": {
        "id": "c0QaKJLG9Ysi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Helper Function: `season_to_numeric`\n",
        "\n",
        "This function converts a season string (e.g., \"2016/17\") into its starting year (e.g., 2016) for easier comparison with transfer seasons."
      ],
      "metadata": {
        "id": "D35GBWQW9bbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def season_to_numeric(season_str):\n",
        "    \"\"\"\n",
        "    Convert season string like '2016/17' to numeric value 2016 for comparison.\n",
        "    Handles non-string inputs gracefully.\n",
        "    \"\"\"\n",
        "    if isinstance(season_str, str) and '/' in season_str:\n",
        "        try:\n",
        "            return int(season_str.split('/')[0])\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "Ya-1wMgi9dI_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Process and Aggregate Each File\n",
        "\n",
        "This loop iterates through each cleaned file, determines the transfer season, splits the data into pre and post-transfer periods based on the 'Season_Year' column, aggregates player stats by averaging them, and collects these aggregated dataframes."
      ],
      "metadata": {
        "id": "LcKekW139fa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file in all_cleaned_files:\n",
        "    print(f\"\\nProcessing aggregation for file: {file}\")\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract transfer season from filename (e.g., \"2018-19_Transfers_...\")\n",
        "    filename = os.path.basename(file)\n",
        "    match = re.match(r\"(\\d{4})-\\d{2}_Transfers_\", filename)\n",
        "    if not match:\n",
        "        print(f\"Skipping {file}: Filename format unexpected.\")\n",
        "        continue\n",
        "\n",
        "    transfer_season_str = match.group(1)\n",
        "    transfer_year = int(transfer_season_str)\n",
        "\n",
        "    print(f\"Identified transfer year: {transfer_year}\")\n",
        "\n",
        "    # Convert 'Season' column in data to a numeric start year (e.g., \"2016/17\" -> 2016)\n",
        "    df[\"Season_Year\"] = df[\"Season\"].apply(season_to_numeric)\n",
        "\n",
        "    # Split into pre and post transfer datasets based on the transfer year\n",
        "    pre_df = df[df[\"Season_Year\"] < transfer_year]\n",
        "    post_df = df[df[\"Season_Year\"] >= transfer_year]\n",
        "\n",
        "    # Aggregate stats per player by taking the mean of all numeric columns\n",
        "    pre_agg = pre_df.groupby(\"Player Name\").mean(numeric_only=True).round(2).reset_index()\n",
        "    post_agg = post_df.groupby(\"Player Name\").mean(numeric_only=True).round(2).reset_index()\n",
        "\n",
        "    # Append the aggregated dataframes to their respective lists\n",
        "    if not pre_agg.empty:\n",
        "        pre_transfer_data.append(pre_agg)\n",
        "        print(f\"Added {len(pre_agg)} pre-transfer player aggregations.\")\n",
        "    else:\n",
        "        print(f\"No pre-transfer data found for {file}.\")\n",
        "\n",
        "    if not post_agg.empty:\n",
        "        post_transfer_data.append(post_agg)\n",
        "        print(f\"Added {len(post_agg)} post-transfer player aggregations.\")\n",
        "    else:\n",
        "        print(f\"No post-transfer data found for {file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZEnDTID9hHs",
        "outputId": "bd16397c-8d26-4d82-8c96-7a4d2afe9019"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing aggregation for file: 2018-19_Transfers_2016-17_to_2019-20_Stats.raw_cleaned_common.csv\n",
            "Identified transfer year: 2018\n",
            "Added 23 pre-transfer player aggregations.\n",
            "Added 23 post-transfer player aggregations.\n",
            "\n",
            "Processing aggregation for file: 2019-20_Transfers_2017-18_to_2020-21_Stats.raw_cleaned_common.csv\n",
            "Identified transfer year: 2019\n",
            "Added 31 pre-transfer player aggregations.\n",
            "Added 31 post-transfer player aggregations.\n",
            "\n",
            "Processing aggregation for file: 2020-21_Transfers_2018-19_to_2021-22_Stats.raw_cleaned_common.csv\n",
            "Identified transfer year: 2020\n",
            "Added 29 pre-transfer player aggregations.\n",
            "Added 29 post-transfer player aggregations.\n",
            "\n",
            "Processing aggregation for file: 2021-22_Transfers_2019-20_to_2022-23_Stats.raw_cleaned_common.csv\n",
            "Identified transfer year: 2021\n",
            "Added 21 pre-transfer player aggregations.\n",
            "Added 21 post-transfer player aggregations.\n",
            "\n",
            "Processing aggregation for file: 2022-23_Transfers_2020-21_to_2023-24_Stats.raw_cleaned_common.csv\n",
            "Identified transfer year: 2022\n",
            "Added 44 pre-transfer player aggregations.\n",
            "Added 44 post-transfer player aggregations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Combine All Aggregated Data and Save\n",
        "\n",
        "Finally, all individual pre-transfer and post-transfer aggregated dataframes are combined. If a player appears in multiple aggregated dataframes (e.g., from different original files), their stats are further averaged to create a single, comprehensive pre-transfer and post-transfer dataset."
      ],
      "metadata": {
        "id": "HwFPs25Q9l7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if pre_transfer_data:\n",
        "    # Concatenate all pre-transfer data and then average stats for players appearing across multiple files\n",
        "    pre_all = pd.concat(pre_transfer_data).groupby(\"Player Name\").mean(numeric_only=True).round(2).reset_index()\n",
        "    print(f\"\\nTotal unique players in pre-transfer dataset: {len(pre_all)}\")\n",
        "else:\n",
        "    pre_all = pd.DataFrame() # Create empty DataFrame if no data\n",
        "    print(\"\\nNo pre-transfer data to combine.\")\n",
        "\n",
        "\n",
        "if post_transfer_data:\n",
        "    # Concatenate all post-transfer data and then average stats for players appearing across multiple files\n",
        "    post_all = pd.concat(post_transfer_data).groupby(\"Player Name\").mean(numeric_only=True).round(2).reset_index()\n",
        "    print(f\"Total unique players in post-transfer dataset: {len(post_all)}\")\n",
        "else:\n",
        "    post_all = pd.DataFrame() # Create empty DataFrame if no data\n",
        "    print(\"No post-transfer data to combine.\")\n",
        "\n",
        "# Define folder path for saving outputs\n",
        "folder_path = \"./\" # Current directory\n",
        "\n",
        "# Save the combined results to CSV files\n",
        "if not pre_all.empty:\n",
        "    pre_all.to_csv(os.path.join(folder_path, \"pre_transfer_dataset.csv\"), index=False)\n",
        "    print(f\"Pre-transfer dataset saved to: {os.path.join(folder_path, 'pre_transfer_dataset.csv')}\")\n",
        "else:\n",
        "    print(\"Pre-transfer dataset not saved as it's empty.\")\n",
        "\n",
        "if not post_all.empty:\n",
        "    post_all.to_csv(os.path.join(folder_path, \"post_transfer_dataset.csv\"), index=False)\n",
        "    print(f\"Post-transfer dataset saved to: {os.path.join(folder_path, 'post_transfer_dataset.csv')}\")\n",
        "else:\n",
        "    print(\"Post-transfer dataset not saved as it's empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-xgYtOl9qCh",
        "outputId": "cc98e72c-7643-4fcf-e885-c872cf58fb38"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total unique players in pre-transfer dataset: 148\n",
            "Total unique players in post-transfer dataset: 148\n",
            "Pre-transfer dataset saved to: ./pre_transfer_dataset.csv\n",
            "Post-transfer dataset saved to: ./post_transfer_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Adding Player Position Information\n",
        "\n",
        "This final section enriches your aggregated player statistics by adding a general player position category. This is crucial for further analysis, as player performance metrics can vary significantly by position. This section processes both `pre_transfer_dataset.csv` and `post_transfer_dataset.csv` to ensure consistent position mapping across both.\n",
        "\n",
        "### 4.1 Define file paths and datasets to process\n",
        "\n",
        "Here, we specify the input file containing player positions and define a list of dictionaries. Each dictionary indicates a statistics file (`stats_file`) and its corresponding desired output file name (`output_file`) after positions are merged."
      ],
      "metadata": {
        "id": "Tk_F_aZxOE64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input file paths\n",
        "positions_file = \"transfer_dataset.cleaned.csv\"\n",
        "\n",
        "# Define a list of dictionaries, each containing input stats file and desired output file\n",
        "datasets_to_process = [\n",
        "    {\n",
        "        \"stats_file\": \"pre_transfer_dataset.csv\", # Your pre-transfer stats file\n",
        "        \"output_file\": \"pre_transfer_with_general_positions.csv\" # Adjusted name for clarity\n",
        "    },\n",
        "    {\n",
        "        \"stats_file\": \"post_transfer_dataset.csv\", # Your post-transfer stats file\n",
        "        \"output_file\": \"post_transfer_with_general_positions.csv\" # Adjusted name for clarity\n",
        "    }\n",
        "]\n",
        "\n",
        "# Load the positions dataset once to avoid redundant reads\n",
        "try:\n",
        "    positions_df = pd.read_csv(positions_file)\n",
        "    print(f\"Loaded positions data from '{positions_file}' with shape: {positions_df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Positions file '{positions_file}' not found. Please ensure it's in the same directory as this notebook.\")\n",
        "    # Exit or raise an error as this file is crucial for the next steps\n",
        "    raise FileNotFoundError(f\"Required file '{positions_file}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading '{positions_file}': {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y18lUru92iw",
        "outputId": "2dddca3c-c512-4de0-f5ad-ed3b34839c62"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded positions data from 'transfer_dataset.cleaned.csv' with shape: (238, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Create Position Mapping Function\n",
        "\n",
        "This function maps detailed player positions (e.g., 'centre-back', 'striker') to broader, more general categories (e.g., 'Defense', 'Attack'). This simplification can be very useful for higher-level analysis."
      ],
      "metadata": {
        "id": "X-PIatkt94-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_position(position):\n",
        "    \"\"\"\n",
        "    Maps a detailed player position to a general position group (Attack, Midfield, Defense).\n",
        "    Returns None if the position does not match any known group.\n",
        "    \"\"\"\n",
        "    if pd.isna(position):\n",
        "        return None\n",
        "    position = str(position).lower()\n",
        "    if any(pos in position for pos in ['striker', 'forward', 'winger', 'attacker']):\n",
        "        return 'Attack'\n",
        "    elif any(pos in position for pos in ['midfielder', 'midfield']):\n",
        "        return 'Midfield'\n",
        "    elif any(pos in position for pos in ['defender', 'back', 'centre-back', 'full-back']):\n",
        "        return 'Defense'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Apply mapping to the 'Position' column in positions_df\n",
        "print(\"Mapping detailed positions to general position groups in the positions dataset...\")\n",
        "positions_df['Position'] = positions_df['Position'].apply(map_position)\n",
        "print(\"Position mapping complete.\")\n",
        "\n",
        "# Review some mapped positions for verification\n",
        "print(\"\\nSample of original and general positions from 'transfer_dataset.cleaned.csv':\")\n",
        "print(positions_df[['Player Name', 'Position', 'Position']].drop_duplicates().head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hljEhSFM9694",
        "outputId": "6d96df09-9a15-4c6b-d8e8-08d2e927fb1a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping detailed positions to general position groups in the positions dataset...\n",
            "Position mapping complete.\n",
            "\n",
            "Sample of original and general positions from 'transfer_dataset.cleaned.csv':\n",
            "        Player Name  Position  Position\n",
            "0    Ante Palaversa  Midfield  Midfield\n",
            "1  Philippe Sandler   Defense   Defense\n",
            "2        Ko Itakura   Defense   Defense\n",
            "3     Daniel Arzani    Attack    Attack\n",
            "4              Fred  Midfield  Midfield\n",
            "5       Diogo Dalot   Defense   Defense\n",
            "6        Ben Gibson   Defense   Defense\n",
            "7          Angelino   Defense   Defense\n",
            "8      Zack Steffen      None      None\n",
            "9    Slobodan Tedic    Attack    Attack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Helper Function: Process and Save Each Dataset\n",
        "\n",
        "This function encapsulates the logic for loading a statistics dataset, merging it with the pre-mapped position data, and saving the result. This makes the main loop cleaner and easier to manage."
      ],
      "metadata": {
        "id": "1tyf_4GO98sO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_save_dataset(stats_filepath, output_filepath, positions_df_mapped):\n",
        "    \"\"\"\n",
        "    Loads a stats dataset, merges it with mapped positions, and saves the result to the same directory.\n",
        "\n",
        "    Args:\n",
        "        stats_filepath (str): Path to the statistics CSV file.\n",
        "        output_filepath (str): Desired name for the output CSV file.\n",
        "        positions_df_mapped (pd.DataFrame): DataFrame containing 'Player Name' and 'Position'.\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing: {stats_filepath}\")\n",
        "    try:\n",
        "        stats_df = pd.read_csv(stats_filepath)\n",
        "        print(f\"  Loaded stats data with shape: {stats_df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Stats file '{stats_filepath}' not found. Skipping this dataset.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading '{stats_filepath}': {e}\")\n",
        "        return\n",
        "\n",
        "    # Merge with the main stats dataset using 'Player Name'\n",
        "    print(f\"  Merging with positions data...\")\n",
        "    merged_df = stats_df.merge(\n",
        "        positions_df_mapped[['Player Name', 'Position']],\n",
        "        on='Player Name',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Check for missing positions after merge\n",
        "    missing_positions_count = merged_df['Position'].isnull().sum()\n",
        "    if missing_positions_count > 0:\n",
        "        print(f\"  ⚠️ Warning: {missing_positions_count} players in '{output_filepath}' have missing 'Position' after merge.\")\n",
        "        print(\"  This may indicate player name mismatches or unmapped positions in 'transfer_dataset.cleaned.csv'.\")\n",
        "    else:\n",
        "        print(f\"  ✅ All players in '{output_filepath}' successfully matched with a 'Position'.\")\n",
        "\n",
        "    print(f\"  Merged DataFrame shape: {merged_df.shape}\")\n",
        "\n",
        "    # Step 4: Save the result to the current directory\n",
        "    merged_df.to_csv(output_filepath, index=False)\n",
        "    print(f\"Successfully saved processed data to: {output_filepath}\")"
      ],
      "metadata": {
        "id": "YpWVv1Ub9-3Y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Process All Defined Datasets\n",
        "\n",
        "This loop iterates through the `datasets_to_process` list and calls the helper function for each, ensuring both pre and post-transfer datasets are enriched with position information and saved."
      ],
      "metadata": {
        "id": "yRpUZAQF-ChS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all datasets defined in the list using the helper function\n",
        "for dataset_info in datasets_to_process:\n",
        "    process_and_save_dataset(\n",
        "        dataset_info[\"stats_file\"],\n",
        "        dataset_info[\"output_file\"],\n",
        "        positions_df\n",
        "    )\n",
        "\n",
        "print(\"\\nAll specified datasets have been processed and saved with general position information.\")\n",
        "print(\"Data cleaning, merging, and position enrichment complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_Nv_g5k-EBW",
        "outputId": "4c1e731e-528b-4ce1-c87f-dfcef32097fe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: pre_transfer_dataset.csv\n",
            "  Loaded stats data with shape: (148, 30)\n",
            "  Merging with positions data...\n",
            "  ⚠️ Warning: 26 players in 'pre_transfer_with_general_positions.csv' have missing 'Position' after merge.\n",
            "  This may indicate player name mismatches or unmapped positions in 'transfer_dataset.cleaned.csv'.\n",
            "  Merged DataFrame shape: (148, 31)\n",
            "Successfully saved processed data to: pre_transfer_with_general_positions.csv\n",
            "\n",
            "Processing: post_transfer_dataset.csv\n",
            "  Loaded stats data with shape: (148, 30)\n",
            "  Merging with positions data...\n",
            "  ⚠️ Warning: 26 players in 'post_transfer_with_general_positions.csv' have missing 'Position' after merge.\n",
            "  This may indicate player name mismatches or unmapped positions in 'transfer_dataset.cleaned.csv'.\n",
            "  Merged DataFrame shape: (148, 31)\n",
            "Successfully saved processed data to: post_transfer_with_general_positions.csv\n",
            "\n",
            "All specified datasets have been processed and saved with general position information.\n",
            "Data cleaning, merging, and position enrichment complete!\n"
          ]
        }
      ]
    }
  ]
}