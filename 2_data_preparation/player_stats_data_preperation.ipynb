{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHkY5J6L8RmN"
   },
   "source": [
    "# Data Cleaning and Preparation for Transfer Analysis\n",
    "\n",
    "This notebook outlines the complete process of cleaning raw football player statistics, handling missing values, standardizing data, and preparing datasets for transfer analysis. The goal is to make the data easily understandable and replicable.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Initial Setup and Data Loading\n",
    "\n",
    "This section imports necessary libraries and defines utility functions for the initial cleaning steps, such as dropping columns with percentage values, extracting player names from URLs, and handling various data types and missing values.\n",
    "\n",
    "### 1.1 Import Libraries\n",
    "\n",
    "We start by importing all the required libraries for data manipulation and regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S3-vH_c7XHwr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmBbMiY78bD7"
   },
   "source": [
    "### 1.2 Define Data Cleaning Functions\n",
    "\n",
    "These functions encapsulate specific cleaning logic, making the main processing loop cleaner and more readable.\n",
    "\n",
    "#### `drop_columns_with_percent(df)`\n",
    "\n",
    "This function identifies and removes columns that contain percentage signs, as these often represent calculated metrics that might not be suitable for direct comparison or aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_XOulOHXC055"
   },
   "outputs": [],
   "source": [
    "def drop_columns_with_percent(df):\n",
    "    \"\"\"\n",
    "    Drops columns from a pandas DataFrame if any of their values (as strings)\n",
    "    contain a '%' sign. This is useful for removing calculated metrics that\n",
    "    might not be directly comparable or are redundant.\n",
    "    \"\"\"\n",
    "    columns_to_drop = [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if df[col].astype(str).str.contains(\"%\", na=False).any()\n",
    "    ]\n",
    "    if columns_to_drop:\n",
    "        print(f\"Dropping columns containing '%': {', '.join(columns_to_drop)}\")\n",
    "        return df.drop(columns=columns_to_drop)\n",
    "    else:\n",
    "        print(\"No columns with '%' found to drop.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n02XR5gh8iLD"
   },
   "source": [
    "#### `extract_and_format_name(url)`\n",
    "\n",
    "This helper function extracts a player's name from a given URL string and formats it into a more readable title-case format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7KdTC67d8kE8"
   },
   "outputs": [],
   "source": [
    "def extract_and_format_name(url):\n",
    "    \"\"\"\n",
    "    Extracts and formats a player's name from a URL.\n",
    "    Example: '.../some-player-name' becomes 'Some Player Name'.\n",
    "    Returns None if the URL is NaN or cannot be parsed.\n",
    "    \"\"\"\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    match = re.search(r\"/([^/]+)$\", url)  # Regex to find string after the last '/'\n",
    "    if match:\n",
    "        # Capitalize each word in the extracted name (e.g., 'some-player-name' -> 'Some Player Name')\n",
    "        return \" \".join(\n",
    "            word.capitalize() for word in match.group(1).replace(\"_\", \"-\").split(\"-\")\n",
    "        )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewBM5eaY8oN1"
   },
   "source": [
    "#### `process_url_column(df)`\n",
    "\n",
    "This function applies the `extract_and_format_name` function to the 'URL' column and renames it to 'Player Name'. This is crucial for uniquely identifying players across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aXcXqsGx8rF7"
   },
   "outputs": [],
   "source": [
    "def process_url_column(df):\n",
    "    \"\"\"\n",
    "    Processes the 'URL' column to extract player names and renames the column to 'Player Name'.\n",
    "    This standardizes player identification.\n",
    "    \"\"\"\n",
    "    if \"URL\" in df.columns:\n",
    "        df = df.copy()\n",
    "        df[\"URL\"] = df[\"URL\"].apply(extract_and_format_name)\n",
    "        df = df.rename(columns={\"URL\": \"Player Name\"})\n",
    "        print(\"Processed 'URL' column to 'Player Name'.\")\n",
    "    else:\n",
    "        print(\"'URL' column not found, skipping URL processing.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgzff3LC8tVc"
   },
   "source": [
    "#### `clean_and_impute(df)`\n",
    "\n",
    "This function handles data type conversions and imputes missing values. It specifically addresses common issues like commas in numeric strings and fills specific columns with zeros, while imputing others with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "3YQRJbdD8vdX"
   },
   "outputs": [],
   "source": [
    "def clean_and_impute(df):\n",
    "    \"\"\"\n",
    "    Performs data type conversion and imputation on the DataFrame.\n",
    "    Specifically handles 'Minutes Played' and 'Possession - Touches' by converting them to numeric,\n",
    "    imputes 'Defending - Penalties conceded' and 'Possession - Penalties awarded' with 0,\n",
    "    and fills other numerical missing values with their respective column medians.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert columns that might contain commas as thousands separators to numeric\n",
    "    columns_to_clean_numeric = [\"Minutes Played\", \"Possession - Touches\"]\n",
    "\n",
    "    for col in columns_to_clean_numeric:\n",
    "        if col in df.columns and df[col].dtype == \"object\":\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col].astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\"\n",
    "            )\n",
    "            print(f\"Converted '{col}' to numeric.\")\n",
    "        elif col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(f\"'{col}' is already numeric.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"'{col}' not found or not an object type, skipping numeric conversion.\"\n",
    "            )\n",
    "\n",
    "    # Impute specific columns with 0, as these often represent counts that were zero if not recorded.\n",
    "    columns_to_impute_zero = [\n",
    "        \"Defending - Penalties conceded\",\n",
    "        \"Possession - Penalties awarded\",\n",
    "    ]\n",
    "    for col in columns_to_impute_zero:\n",
    "        if col in df.columns and df[col].isnull().sum() > 0:\n",
    "            nan_count = df[col].isnull().sum()\n",
    "            df[col] = df[col].fillna(0)\n",
    "            print(f\"Imputed {nan_count} missing values in '{col}' with 0.\")\n",
    "        elif col in df.columns:\n",
    "            print(f\"'{col}' has no missing values.\")\n",
    "        else:\n",
    "            print(f\"'{col}' not found, skipping zero imputation.\")\n",
    "\n",
    "    # Impute remaining numerical columns with their median to preserve distribution shape and reduce outlier impact.\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and df[col].isnull().sum() > 0:\n",
    "            nan_count = df[col].isnull().sum()\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            print(\n",
    "                f\"Imputed {nan_count} missing values in '{col}' with median ({median_val}).\"\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZazmPAs84OB"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Processing Raw Datasets\n",
    "\n",
    "This section iterates through the raw CSV files, applies the defined cleaning functions, and ensures all datasets have a consistent set of columns before saving them.\n",
    "\n",
    "### 2.1 Define File Paths\n",
    "\n",
    "List all the raw CSV files that need to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5aaYi8B85M2"
   },
   "outputs": [],
   "source": [
    "# List of CSV files to process\n",
    "csv_file_paths = [\n",
    "    \"../1_datasets/raw/2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv\",\n",
    "    \"../1_datasets/raw/2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv\",\n",
    "    \"../1_datasets/raw/2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv\",\n",
    "    \"../1_datasets/raw/2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv\",\n",
    "    \"../1_datasets/raw/2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv\",\n",
    "]\n",
    "\n",
    "processed_dfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru_EhAOd898I"
   },
   "source": [
    "### 2.2 Loop Through Files and Apply Cleaning\n",
    "\n",
    "Each file is read, cleaned using the defined functions, and then added to a list of processed DataFrames. Error handling is included to catch missing files or other processing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYWrJLkY9AMY",
    "outputId": "9cc5d7ef-ddf8-4da9-d57e-0550226097b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing file: ../1_datasets/raw/2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv ==========\n",
      "Original shape: (84, 43)\n",
      "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
      "'URL' column not found, skipping URL processing.\n",
      "Converted 'Minutes Played' to numeric.\n",
      "'Possession - Touches' is already numeric.\n",
      "Imputed 64 missing values in 'Defending - Penalties conceded' with 0.\n",
      "Imputed 72 missing values in 'Possession - Penalties awarded' with 0.\n",
      "Imputed 7 missing values in 'Defending - Aerial duels won' with median (1.36).\n",
      "Imputed 9 missing values in 'Defending - Blocked' with median (0.29).\n",
      "Imputed 4 missing values in 'Defending - Dribbled past' with median (0.815).\n",
      "Imputed 3 missing values in 'Defending - Duels won' with median (5.29).\n",
      "Imputed 4 missing values in 'Defending - Fouls committed' with median (1.085).\n",
      "Imputed 5 missing values in 'Defending - Interceptions' with median (1.13).\n",
      "Imputed 12 missing values in 'Defending - Possession won final 3rd' with median (0.36).\n",
      "Imputed 3 missing values in 'Defending - Recoveries' with median (5.16).\n",
      "Imputed 3 missing values in 'Defending - Tackles won' with median (0.97).\n",
      "Imputed 3 missing values in 'Discipline - Red cards' with median (0.0).\n",
      "Imputed 3 missing values in 'Discipline - Yellow cards' with median (0.15).\n",
      "Imputed 3 missing values in 'Minutes Played' with median (2014.0).\n",
      "Imputed 5 missing values in 'Passing - Accurate long balls' with median (2.32).\n",
      "Imputed 3 missing values in 'Passing - Assists' with median (0.07).\n",
      "Imputed 5 missing values in 'Passing - Chances created' with median (0.96).\n",
      "Imputed 17 missing values in 'Passing - Successful crosses' with median (0.43).\n",
      "Imputed 3 missing values in 'Passing - Successful passes' with median (30.45).\n",
      "Imputed 3 missing values in 'Possession - Dispossessed' with median (1.03).\n",
      "Imputed 4 missing values in 'Possession - Fouls won' with median (1.1349999999999998).\n",
      "Imputed 8 missing values in 'Possession - Successful dribbles' with median (1.095).\n",
      "Imputed 3 missing values in 'Possession - Touches' with median (58.18).\n",
      "Imputed 4 missing values in 'Possession - Touches in opposition box' with median (1.395).\n",
      "Imputed 3 missing values in 'ShootingRank - Goals' with median (0.07).\n",
      "Imputed 3 missing values in 'ShootingRank - Shots' with median (1.0).\n",
      "Imputed 7 missing values in 'ShootingRank - Shots on target' with median (0.31).\n",
      "Successfully processed ../1_datasets/raw/2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv. Cleaned shape: (84, 30)\n",
      "\n",
      "========== Processing file: ../1_datasets/raw/2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv ==========\n",
      "Original shape: (115, 44)\n",
      "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
      "Processed 'URL' column to 'Player Name'.\n",
      "Converted 'Minutes Played' to numeric.\n",
      "Converted 'Possession - Touches' to numeric.\n",
      "Imputed 100 missing values in 'Defending - Penalties conceded' with 0.\n",
      "Imputed 87 missing values in 'Possession - Penalties awarded' with 0.\n",
      "Imputed 8 missing values in 'Defending - Aerial duels won' with median (1.34).\n",
      "Imputed 13 missing values in 'Defending - Blocked' with median (0.455).\n",
      "Imputed 6 missing values in 'Defending - Dribbled past' with median (0.89).\n",
      "Imputed 5 missing values in 'Defending - Duels won' with median (5.385).\n",
      "Imputed 7 missing values in 'Defending - Fouls committed' with median (1.165).\n",
      "Imputed 8 missing values in 'Defending - Interceptions' with median (0.87).\n",
      "Imputed 11 missing values in 'Defending - Possession won final 3rd' with median (0.55).\n",
      "Imputed 5 missing values in 'Defending - Recoveries' with median (4.99).\n",
      "Imputed 5 missing values in 'Defending - Tackles won' with median (0.865).\n",
      "Imputed 5 missing values in 'Discipline - Red cards' with median (0.0).\n",
      "Imputed 5 missing values in 'Discipline - Yellow cards' with median (0.12).\n",
      "Imputed 5 missing values in 'Minutes Played' with median (2078.5).\n",
      "Imputed 8 missing values in 'Passing - Accurate long balls' with median (1.23).\n",
      "Imputed 5 missing values in 'Passing - Assists' with median (0.09).\n",
      "Imputed 6 missing values in 'Passing - Chances created' with median (1.0).\n",
      "Imputed 24 missing values in 'Passing - Successful crosses' with median (0.37).\n",
      "Imputed 5 missing values in 'Passing - Successful passes' with median (25.7).\n",
      "Imputed 5 missing values in 'Possession - Dispossessed' with median (1.39).\n",
      "Imputed 7 missing values in 'Possession - Fouls won' with median (1.165).\n",
      "Imputed 7 missing values in 'Possession - Successful dribbles' with median (1.005).\n",
      "Imputed 5 missing values in 'Possession - Touches' with median (52.65).\n",
      "Imputed 7 missing values in 'Possession - Touches in opposition box' with median (3.185).\n",
      "Imputed 5 missing values in 'ShootingRank - Goals' with median (0.125).\n",
      "Imputed 89 missing values in 'ShootingRank - Penalty goals' with median (0.07).\n",
      "Imputed 5 missing values in 'ShootingRank - Shots' with median (1.725).\n",
      "Imputed 15 missing values in 'ShootingRank - Shots on target' with median (0.655).\n",
      "Successfully processed ../1_datasets/raw/2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv. Cleaned shape: (115, 31)\n",
      "\n",
      "========== Processing file: ../1_datasets/raw/2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv ==========\n",
      "Original shape: (106, 48)\n",
      "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
      "'URL' column not found, skipping URL processing.\n",
      "Converted 'Minutes Played' to numeric.\n",
      "'Possession - Touches' is already numeric.\n",
      "Imputed 76 missing values in 'Defending - Penalties conceded' with 0.\n",
      "Imputed 79 missing values in 'Possession - Penalties awarded' with 0.\n",
      "Imputed 1 missing values in 'Average Rating' with median (6.88).\n",
      "Imputed 17 missing values in 'Defending - Blocked' with median (0.2).\n",
      "Imputed 3 missing values in 'Defending - Dribbled past' with median (0.66).\n",
      "Imputed 1 missing values in 'Defending - Fouls committed' with median (1.01).\n",
      "Imputed 1 missing values in 'Defending - Interceptions' with median (0.96).\n",
      "Imputed 16 missing values in 'Defending - Possession won final 3rd' with median (0.37).\n",
      "Imputed 1 missing values in 'Defending - Tackles won' with median (0.92).\n",
      "Imputed 1 missing values in 'Passing - Accurate long balls' with median (1.64).\n",
      "Imputed 6 missing values in 'Passing - Chances created' with median (0.74).\n",
      "Imputed 49 missing values in 'Passing - Expected assists (xA)' with median (0.04).\n",
      "Imputed 27 missing values in 'Passing - Successful crosses' with median (0.31).\n",
      "Imputed 3 missing values in 'Possession - Fouls won' with median (0.9).\n",
      "Imputed 7 missing values in 'Possession - Successful dribbles' with median (0.89).\n",
      "Imputed 2 missing values in 'Possession - Touches in opposition box' with median (1.46).\n",
      "Imputed 51 missing values in 'ShootingRank - Expected goals (xG)' with median (0.04).\n",
      "Imputed 93 missing values in 'ShootingRank - Penalty goals' with median (0.1).\n",
      "Imputed 11 missing values in 'ShootingRank - Shots on target' with median (0.31).\n",
      "Imputed 59 missing values in 'ShootingRank - xG on target (xGOT)' with median (0.05).\n",
      "Imputed 106 missing values in 'Unnamed: 47' with median (nan).\n",
      "Successfully processed ../1_datasets/raw/2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv. Cleaned shape: (106, 35)\n",
      "\n",
      "========== Processing file: ../1_datasets/raw/2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv ==========\n",
      "Original shape: (76, 47)\n",
      "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
      "Processed 'URL' column to 'Player Name'.\n",
      "Converted 'Minutes Played' to numeric.\n",
      "'Possession - Touches' is already numeric.\n",
      "Imputed 64 missing values in 'Defending - Penalties conceded' with 0.\n",
      "Imputed 63 missing values in 'Possession - Penalties awarded' with 0.\n",
      "Imputed 2 missing values in 'Average Rating' with median (6.86).\n",
      "Imputed 9 missing values in 'Defending - Blocked' with median (0.43).\n",
      "Imputed 5 missing values in 'Defending - Dribbled past' with median (0.76).\n",
      "Imputed 7 missing values in 'Defending - Interceptions' with median (0.82).\n",
      "Imputed 4 missing values in 'Defending - Possession won final 3rd' with median (0.565).\n",
      "Imputed 1 missing values in 'Defending - Tackles won' with median (0.99).\n",
      "Imputed 3 missing values in 'Passing - Accurate long balls' with median (1.36).\n",
      "Imputed 1 missing values in 'Passing - Chances created' with median (1.02).\n",
      "Imputed 18 missing values in 'Passing - Expected assists (xA)' with median (0.08).\n",
      "Imputed 15 missing values in 'Passing - Successful crosses' with median (0.21).\n",
      "Imputed 2 missing values in 'Possession - Fouls won' with median (1.25).\n",
      "Imputed 19 missing values in 'ShootingRank - Expected goals (xG)' with median (0.11).\n",
      "Imputed 19 missing values in 'ShootingRank - Non-penalty xG' with median (0.11).\n",
      "Imputed 64 missing values in 'ShootingRank - Penalty goals' with median (0.065).\n",
      "Imputed 8 missing values in 'ShootingRank - Shots on target' with median (0.645).\n",
      "Successfully processed ../1_datasets/raw/2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv. Cleaned shape: (76, 34)\n",
      "\n",
      "========== Processing file: ../1_datasets/raw/2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv ==========\n",
      "Original shape: (167, 48)\n",
      "Dropping columns containing '%': Defending - Aerial duels won %, Defending - Duels won %, Defending - Tackles won %, Passing - Cross accuracy, Passing - Long ball accuracy, Passing - Pass accuracy, Possession - Dribble success, ShootingRank - xG on target (xGOT), Trait - Aerial duels won, Trait - Chances created, Trait - Defensive actions, Trait - Goals, Trait - Shot attempts, Trait - Touches\n",
      "Processed 'URL' column to 'Player Name'.\n",
      "Converted 'Minutes Played' to numeric.\n",
      "'Possession - Touches' is already numeric.\n",
      "Imputed 144 missing values in 'Defending - Penalties conceded' with 0.\n",
      "Imputed 129 missing values in 'Possession - Penalties awarded' with 0.\n",
      "Imputed 3 missing values in 'Defending - Aerial duels won' with median (1.35).\n",
      "Imputed 17 missing values in 'Defending - Blocked' with median (0.365).\n",
      "Imputed 6 missing values in 'Defending - Dribbled past' with median (0.79).\n",
      "Imputed 5 missing values in 'Defending - Fouls committed' with median (1.08).\n",
      "Imputed 8 missing values in 'Defending - Interceptions' with median (0.77).\n",
      "Imputed 12 missing values in 'Defending - Possession won final 3rd' with median (0.58).\n",
      "Imputed 3 missing values in 'Defending - Tackles won' with median (0.98).\n",
      "Imputed 7 missing values in 'Passing - Accurate long balls' with median (1.245).\n",
      "Imputed 5 missing values in 'Passing - Chances created' with median (0.995).\n",
      "Imputed 6 missing values in 'Passing - Expected assists (xA)' with median (0.09).\n",
      "Imputed 45 missing values in 'Passing - Successful crosses' with median (0.355).\n",
      "Imputed 3 missing values in 'Possession - Fouls won' with median (1.2650000000000001).\n",
      "Imputed 6 missing values in 'Possession - Successful dribbles' with median (1.2).\n",
      "Imputed 1 missing values in 'Possession - Touches in opposition box' with median (2.615).\n",
      "Imputed 10 missing values in 'ShootingRank - Expected goals (xG)' with median (0.13).\n",
      "Imputed 10 missing values in 'ShootingRank - Non-penalty xG' with median (0.12).\n",
      "Imputed 114 missing values in 'ShootingRank - Penalty goals' with median (1.02).\n",
      "Imputed 4 missing values in 'ShootingRank - Shots' with median (1.04).\n",
      "Imputed 18 missing values in 'ShootingRank - Shots on target' with median (0.36).\n",
      "Successfully processed ../1_datasets/raw/2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv. Cleaned shape: (167, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khesraw/Library/Python/3.9/lib/python/site-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "for file_path in csv_file_paths:\n",
    "    print(f\"\\n{'=' * 10} Processing file: {file_path} {'=' * 10}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "        df_cleaned = drop_columns_with_percent(df)\n",
    "        df_cleaned = process_url_column(df_cleaned)\n",
    "        df_cleaned = clean_and_impute(df_cleaned)\n",
    "\n",
    "        processed_dfs.append(df_cleaned)\n",
    "        print(f\"Successfully processed {file_path}. Cleaned shape: {df_cleaned.shape}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: The file '{file_path}' was not found. Please ensure it's in the correct directory.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQPunNLZ9C_h"
   },
   "source": [
    "### 2.3 Identify Common Columns and Save Cleaned Data\n",
    "\n",
    "After all files are individually cleaned, this step identifies the common columns across all processed DataFrames. This is crucial for creating a unified dataset where players' statistics can be directly compared. Each cleaned DataFrame is then filtered to include only these common columns and saved to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bv9EqUc09Fyf",
    "outputId": "cabfe4be-1700-41ae-a4af-d6ce487f1c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 30 common columns across all datasets.\n",
      "Common columns found:\n",
      "['Player Name', 'Season', 'Average Rating', 'Defending - Aerial duels won', 'Defending - Blocked', 'Defending - Dribbled past', 'Defending - Duels won', 'Defending - Fouls committed', 'Defending - Interceptions', 'Defending - Penalties conceded', 'Defending - Possession won final 3rd', 'Defending - Recoveries', 'Defending - Tackles won', 'Discipline - Red cards', 'Discipline - Yellow cards', 'Minutes Played', 'Passing - Accurate long balls', 'Passing - Assists', 'Passing - Chances created', 'Passing - Successful crosses', 'Passing - Successful passes', 'Possession - Dispossessed', 'Possession - Fouls won', 'Possession - Penalties awarded', 'Possession - Successful dribbles', 'Possession - Touches', 'Possession - Touches in opposition box', 'ShootingRank - Goals', 'ShootingRank - Shots', 'ShootingRank - Shots on target']\n",
      "\n",
      "âœ… All missing values handled for ../1_datasets/raw/2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv.\n",
      "\n",
      "âœ… Stored 1 cleaned DataFrames in memory for further processing.\n",
      "\n",
      "âœ… All missing values handled for ../1_datasets/raw/2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv.\n",
      "\n",
      "âœ… Stored 2 cleaned DataFrames in memory for further processing.\n",
      "\n",
      "âœ… All missing values handled for ../1_datasets/raw/2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv.\n",
      "\n",
      "âœ… Stored 3 cleaned DataFrames in memory for further processing.\n",
      "\n",
      "âœ… All missing values handled for ../1_datasets/raw/2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv.\n",
      "\n",
      "âœ… Stored 4 cleaned DataFrames in memory for further processing.\n",
      "\n",
      "âœ… All missing values handled for ../1_datasets/raw/2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv.\n",
      "\n",
      "âœ… Stored 5 cleaned DataFrames in memory for further processing.\n",
      "\n",
      "ðŸ“Œ Sample from the first cleaned DataFrame:\n",
      "        Player Name   Season  Average Rating  Defending - Aerial duels won  \\\n",
      "0  Philippe Sandler  2016/17            6.48                          1.36   \n",
      "1              Fred  2016/17            6.78                          1.36   \n",
      "2        Ben Gibson  2016/17            7.00                          1.61   \n",
      "3           Bernard  2016/17            6.91                          1.36   \n",
      "4       Jonny Evans  2016/17            7.03                          1.54   \n",
      "\n",
      "   Defending - Blocked  Defending - Dribbled past  Defending - Duels won  \\\n",
      "0                 0.29                      0.815                   5.29   \n",
      "1                 0.19                      2.120                   5.02   \n",
      "2                 0.11                      0.210                   3.21   \n",
      "3                 0.93                      0.930                   5.56   \n",
      "4                 0.07                      0.610                   4.64   \n",
      "\n",
      "   Defending - Fouls committed  Defending - Interceptions  \\\n",
      "0                        1.085                       1.13   \n",
      "1                        2.120                       1.55   \n",
      "2                        0.500                       1.50   \n",
      "3                        1.480                       1.67   \n",
      "4                        1.060                       2.52   \n",
      "\n",
      "   Defending - Penalties conceded  ...  Passing - Successful passes  \\\n",
      "0                             0.0  ...                        30.45   \n",
      "1                             0.0  ...                        55.24   \n",
      "2                             0.0  ...                        37.45   \n",
      "3                             0.0  ...                        37.04   \n",
      "4                             0.0  ...                        25.83   \n",
      "\n",
      "   Possession - Dispossessed  Possession - Fouls won  \\\n",
      "0                       1.03                   1.135   \n",
      "1                       2.12                   1.930   \n",
      "2                       0.21                   0.390   \n",
      "3                       1.48                   2.590   \n",
      "4                       0.27                   0.920   \n",
      "\n",
      "   Possession - Penalties awarded  Possession - Successful dribbles  \\\n",
      "0                             0.0                             1.095   \n",
      "1                             0.0                             1.550   \n",
      "2                             0.0                             0.450   \n",
      "3                             0.0                             1.670   \n",
      "4                             0.0                             0.780   \n",
      "\n",
      "   Possession - Touches  Possession - Touches in opposition box  \\\n",
      "0                 58.18                                   1.395   \n",
      "1                 83.05                                   0.970   \n",
      "2                 56.58                                   0.550   \n",
      "3                 64.07                                   2.590   \n",
      "4                 46.94                                   0.440   \n",
      "\n",
      "   ShootingRank - Goals  ShootingRank - Shots  ShootingRank - Shots on target  \n",
      "0                  0.07                  1.00                            0.31  \n",
      "1                  0.19                  1.35                            0.77  \n",
      "2                  0.03                  0.39                            0.16  \n",
      "3                  0.37                  2.04                            0.37  \n",
      "4                  0.07                  0.31                            0.17  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "all_cleaned_files = []  # New list to hold all cleaned files so far\n",
    "\n",
    "if processed_dfs:\n",
    "    # Find the common columns across all processed DataFrames\n",
    "    common_columns = list(\n",
    "        reduce(\n",
    "            lambda left, right: left.intersection(right),\n",
    "            [df.columns for df in processed_dfs],\n",
    "        )\n",
    "    )\n",
    "    print(f\"\\nFound {len(common_columns)} common columns across all datasets.\")\n",
    "    print(\"Common columns found:\")\n",
    "    print(common_columns)\n",
    "\n",
    "    # Filter each DataFrame to keep only the common columns and save\n",
    "    for i, df in enumerate(processed_dfs):\n",
    "        # file_path = csv_file_paths[i]\n",
    "        df_final = df[common_columns].copy()\n",
    "\n",
    "        # Verify no missing values remain\n",
    "        missing_after_all_steps = df_final.isnull().sum()\n",
    "        if missing_after_all_steps.sum() == 0:\n",
    "            print(f\"\\nâœ… All missing values handled for {csv_file_paths[i]}.\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ Warning: Missing values still remain in {csv_file_paths[i]}\")\n",
    "            print(missing_after_all_steps[missing_after_all_steps > 0])\n",
    "\n",
    "        all_cleaned_files.append(df_final)\n",
    "\n",
    "        print(\n",
    "            f\"\\nâœ… Stored {len(all_cleaned_files)} cleaned DataFrames in memory for further processing.\"\n",
    "        )\n",
    "\n",
    "        #   Show a sample from one of the cleaned DataFrames\n",
    "    if all_cleaned_files:\n",
    "        print(\"\\nðŸ“Œ Sample from the first cleaned DataFrame:\")\n",
    "        print(all_cleaned_files[0].head())\n",
    "else:\n",
    "    print(\"\\nNo dataframes were processed. Please check file paths and previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6QRtYpiJs5P"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Merging Datasets into Pre and Post Transfer Aggregations\n",
    "\n",
    "This section focuses on combining the cleaned individual season datasets into two main datasets: one representing player statistics *before* a transfer season and another representing statistics *after* a transfer season. This is crucial for analyzing the impact of transfers.\n",
    "\n",
    "### 3.1 Define file paths for cleaned data\n",
    "\n",
    "List the paths to the newly created cleaned and common column CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c0QaKJLG9Ysi"
   },
   "outputs": [],
   "source": [
    "# all_cleaned_files = [\n",
    "#     \"../1_datasets/cleaned/2018-19_Transfers_2016-17_to_2019-20_Stats.raw_cleaned_common.csv\",\n",
    "#     \"../1_datasets/cleaned/2019-20_Transfers_2017-18_to_2020-21_Stats.raw_cleaned_common.csv\",\n",
    "#     \"../1_datasets/cleaned/2020-21_Transfers_2018-19_to_2021-22_Stats.raw_cleaned_common.csv\",\n",
    "#     \"../1_datasets/cleaned/2021-22_Transfers_2019-20_to_2022-23_Stats.raw_cleaned_common.csv\",\n",
    "#     \"../1_datasets/cleaned/2022-23_Transfers_2020-21_to_2023-24_Stats.raw_cleaned_common.csv\",\n",
    "# ]\n",
    "\n",
    "# Initialize empty lists to collect all pre and post transfer dataframes\n",
    "pre_transfer_data = []\n",
    "post_transfer_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D35GBWQW9bbc"
   },
   "source": [
    "### 3.2 Helper Function: `season_to_numeric`\n",
    "\n",
    "This function converts a season string (e.g., \"2016/17\") into its starting year (e.g., 2016) for easier comparison with transfer seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ya-1wMgi9dI_"
   },
   "outputs": [],
   "source": [
    "def season_to_numeric(season_str):\n",
    "    \"\"\"\n",
    "    Convert season string like '2016/17' to numeric value 2016 for comparison.\n",
    "    Handles non-string inputs gracefully.\n",
    "    \"\"\"\n",
    "    if isinstance(season_str, str) and \"/\" in season_str:\n",
    "        try:\n",
    "            return int(season_str.split(\"/\")[0])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcKekW139fa1"
   },
   "source": [
    "### 3.3 Process and Aggregate Each File\n",
    "\n",
    "This loop iterates through each cleaned file, determines the transfer season, splits the data into pre and post-transfer periods based on the 'Season_Year' column, aggregates player stats by averaging them, and collects these aggregated dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZEnDTID9hHs",
    "outputId": "bd16397c-8d26-4d82-8c96-7a4d2afe9019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing aggregation for file: 2018-19_Transfers_2016-17_to_2019-20_Stats.raw.csv\n",
      "Identified transfer year: 2018\n",
      "Added 23 pre-transfer player aggregations.\n",
      "Added 23 post-transfer player aggregations.\n",
      "\n",
      "Processing aggregation for file: 2019-20_Transfers_2017-18_to_2020-21_Stats.raw.csv\n",
      "Identified transfer year: 2019\n",
      "Added 31 pre-transfer player aggregations.\n",
      "Added 31 post-transfer player aggregations.\n",
      "\n",
      "Processing aggregation for file: 2020-21_Transfers_2018-19_to_2021-22_Stats.raw.csv\n",
      "Identified transfer year: 2020\n",
      "Added 29 pre-transfer player aggregations.\n",
      "Added 29 post-transfer player aggregations.\n",
      "\n",
      "Processing aggregation for file: 2021-22_Transfers_2019-20_to_2022-23_Stats.raw.csv\n",
      "Identified transfer year: 2021\n",
      "Added 21 pre-transfer player aggregations.\n",
      "Added 21 post-transfer player aggregations.\n",
      "\n",
      "Processing aggregation for file: 2022-23_Transfers_2020-21_to_2023-24_Stats.raw.csv\n",
      "Identified transfer year: 2022\n",
      "Added 44 pre-transfer player aggregations.\n",
      "Added 44 post-transfer player aggregations.\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(all_cleaned_files):\n",
    "    # Get the corresponding file path to extract transfer year\n",
    "    file_path = csv_file_paths[i]\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"\\nProcessing aggregation for file: {filename}\")\n",
    "    match = re.match(r\"(\\d{4})-\\d{2}_Transfers_\", filename)\n",
    "    if not match:\n",
    "        print(f\"Skipping {file_path}: Filename format unexpected.\")\n",
    "        continue\n",
    "\n",
    "    transfer_season_str = match.group(1)\n",
    "    transfer_year = int(transfer_season_str)\n",
    "\n",
    "    print(f\"Identified transfer year: {transfer_year}\")\n",
    "\n",
    "    # Convert 'Season' column in data to a numeric start year (e.g., \"2016/17\" -> 2016)\n",
    "    df[\"Season_Year\"] = df[\"Season\"].apply(season_to_numeric)\n",
    "\n",
    "    # Split into pre and post transfer datasets based on the transfer year\n",
    "    pre_df = df[df[\"Season_Year\"] < transfer_year]\n",
    "    post_df = df[df[\"Season_Year\"] >= transfer_year]\n",
    "\n",
    "    # Aggregate stats per player by taking the mean of all numeric columns\n",
    "    pre_agg = (\n",
    "        pre_df.groupby(\"Player Name\").mean(numeric_only=True).round(2).reset_index()\n",
    "    )\n",
    "    post_agg = (\n",
    "        post_df.groupby(\"Player Name\").mean(numeric_only=True).round(2).reset_index()\n",
    "    )\n",
    "\n",
    "    # Append the aggregated dataframes to their respective lists\n",
    "    if not pre_agg.empty:\n",
    "        pre_transfer_data.append(pre_agg)\n",
    "        print(f\"Added {len(pre_agg)} pre-transfer player aggregations.\")\n",
    "    else:\n",
    "        print(f\"No pre-transfer data found for file index {i}.\")\n",
    "\n",
    "    if not post_agg.empty:\n",
    "        post_transfer_data.append(post_agg)\n",
    "        print(f\"Added {len(post_agg)} post-transfer player aggregations.\")\n",
    "    else:\n",
    "        print(f\"No post-transfer data found for file index {i}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwFPs25Q9l7g"
   },
   "source": [
    "### 3.4 Combine All Aggregated Data and Save\n",
    "\n",
    "Finally, all individual pre-transfer and post-transfer aggregated dataframes are combined. If a player appears in multiple aggregated dataframes (e.g., from different original files), their stats are further averaged to create a single, comprehensive pre-transfer and post-transfer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-xgYtOl9qCh",
    "outputId": "cc98e72c-7643-4fcf-e885-c872cf58fb38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique players in pre-transfer dataset: 148\n",
      "Total unique players in post-transfer dataset: 148\n"
     ]
    }
   ],
   "source": [
    "if pre_transfer_data:\n",
    "    # Concatenate all pre-transfer data and then average stats for players appearing across multiple files\n",
    "    pre_all = (\n",
    "        pd.concat(pre_transfer_data)\n",
    "        .groupby(\"Player Name\")\n",
    "        .mean(numeric_only=True)\n",
    "        .round(2)\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(f\"\\nTotal unique players in pre-transfer dataset: {len(pre_all)}\")\n",
    "else:\n",
    "    pre_all = pd.DataFrame()  # Create empty DataFrame if no data\n",
    "    print(\"\\nNo pre-transfer data to combine.\")\n",
    "\n",
    "\n",
    "if post_transfer_data:\n",
    "    # Concatenate all post-transfer data and then average stats for players appearing across multiple files\n",
    "    post_all = (\n",
    "        pd.concat(post_transfer_data)\n",
    "        .groupby(\"Player Name\")\n",
    "        .mean(numeric_only=True)\n",
    "        .round(2)\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(f\"Total unique players in post-transfer dataset: {len(post_all)}\")\n",
    "else:\n",
    "    post_all = pd.DataFrame()  # Create empty DataFrame if no data\n",
    "    print(\"No post-transfer data to combine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk_F_aZxOE64"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Adding Player Position Information\n",
    "\n",
    "This final section enriches your aggregated player statistics by adding a general player position category. This is crucial for further analysis, as player performance metrics can vary significantly by position. This section processes both `pre_transfer_dataset.csv` and `post_transfer_dataset.csv` to ensure consistent position mapping across both.\n",
    "\n",
    "### 4.1 Define file paths and datasets to process\n",
    "\n",
    "Here, we specify the input file containing player positions and define a list of dictionaries. Each dictionary indicates a statistics file (`stats_file`) and its corresponding desired output file name (`output_file`) after positions are merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Y18lUru92iw",
    "outputId": "2dddca3c-c512-4de0-f5ad-ed3b34839c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded positions data from '../1_datasets/cleaned/transfer_dataset.cleaned.csv' with shape: (238, 8)\n"
     ]
    }
   ],
   "source": [
    "# Define input file paths\n",
    "positions_file = \"../1_datasets/cleaned/transfer_dataset.cleaned.csv\"\n",
    "\n",
    "# Define a list of dictionaries, each containing input stats file and desired output file\n",
    "datasets_to_process = [\n",
    "    {\n",
    "        \"data\": pre_all,  # Pre-transfer stats file\n",
    "        \"output_file\": \"../1_datasets/cleaned/pre_transfer.cleaned.csv\",\n",
    "    },\n",
    "    {\n",
    "        \"data\": post_all,  # Post-transfer stats file\n",
    "        \"output_file\": \"../1_datasets/cleaned/post_transfer.cleaned.csv\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load the positions dataset once to avoid redundant reads\n",
    "try:\n",
    "    positions_df = pd.read_csv(positions_file)\n",
    "    print(\n",
    "        f\"Loaded positions data from '{positions_file}' with shape: {positions_df.shape}\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: Positions file '{positions_file}' not found. Please ensure it's in the same directory as this notebook.\"\n",
    "    )\n",
    "    # Exit or raise an error as this file is crucial for the next steps\n",
    "    raise FileNotFoundError(f\"Required file '{positions_file}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading '{positions_file}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-PIatkt94-v"
   },
   "source": [
    "### 4.2 Create Position Mapping Function\n",
    "\n",
    "This function maps detailed player positions (e.g., 'centre-back', 'striker') to broader, more general categories (e.g., 'Defense', 'Attack'). This simplification can be very useful for higher-level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hljEhSFM9694",
    "outputId": "6d96df09-9a15-4c6b-d8e8-08d2e927fb1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping detailed positions to general position groups in the positions dataset...\n",
      "Position mapping complete.\n",
      "\n",
      "Sample of original and general positions from 'transfer_dataset.cleaned.csv':\n",
      "        Player Name  Position  Position\n",
      "0    Ante Palaversa  Midfield  Midfield\n",
      "1  Philippe Sandler   Defense   Defense\n",
      "2        Ko Itakura   Defense   Defense\n",
      "3     Daniel Arzani    Attack    Attack\n",
      "4              Fred  Midfield  Midfield\n",
      "5       Diogo Dalot   Defense   Defense\n",
      "6        Ben Gibson   Defense   Defense\n",
      "7          Angelino   Defense   Defense\n",
      "8      Zack Steffen      None      None\n",
      "9    Slobodan Tedic    Attack    Attack\n"
     ]
    }
   ],
   "source": [
    "def map_position(position):\n",
    "    \"\"\"\n",
    "    Maps a detailed player position to a general position group (Attack, Midfield, Defense).\n",
    "    Returns None if the position does not match any known group.\n",
    "    \"\"\"\n",
    "    if pd.isna(position):\n",
    "        return None\n",
    "    position = str(position).lower()\n",
    "    if any(pos in position for pos in [\"striker\", \"forward\", \"winger\", \"attacker\"]):\n",
    "        return \"Attack\"\n",
    "    elif any(pos in position for pos in [\"midfielder\", \"midfield\"]):\n",
    "        return \"Midfield\"\n",
    "    elif any(\n",
    "        pos in position for pos in [\"defender\", \"back\", \"centre-back\", \"full-back\"]\n",
    "    ):\n",
    "        return \"Defense\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Apply mapping to the 'Position' column in positions_df\n",
    "print(\n",
    "    \"Mapping detailed positions to general position groups in the positions dataset...\"\n",
    ")\n",
    "positions_df[\"Position\"] = positions_df[\"Position\"].apply(map_position)\n",
    "print(\"Position mapping complete.\")\n",
    "\n",
    "# Review some mapped positions for verification\n",
    "print(\"\\nSample of original and general positions from 'transfer_dataset.cleaned.csv':\")\n",
    "print(positions_df[[\"Player Name\", \"Position\", \"Position\"]].drop_duplicates().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tyf_4GO98sO"
   },
   "source": [
    "### 4.3 Helper Function: Process and Save Each Dataset\n",
    "\n",
    "This function encapsulates the logic for loading a statistics dataset, merging it with the pre-mapped position data, and saving the result. This makes the main loop cleaner and easier to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YpWVv1Ub9-3Y"
   },
   "outputs": [],
   "source": [
    "def process_and_save_dataset(stats_filepath, output_filepath, positions_df_mapped):\n",
    "    \"\"\"\n",
    "    Loads a stats dataset, merges it with mapped positions, and saves the result to the same directory.\n",
    "\n",
    "    Args:\n",
    "        stats_filepath (str): Path to the statistics CSV file.\n",
    "        output_filepath (str): Desired name for the output CSV file.\n",
    "        positions_df_mapped (pd.DataFrame): DataFrame containing 'Player Name' and 'Position'.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing: {stats_filepath}\")\n",
    "    try:\n",
    "        stats_df = pd.read_csv(stats_filepath)\n",
    "        print(f\"  Loaded stats data with shape: {stats_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Stats file '{stats_filepath}' not found. Skipping this dataset.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading '{stats_filepath}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Merge with the main stats dataset using 'Player Name'\n",
    "    print(\"Merging with positions data...\")\n",
    "    merged_df = stats_df.merge(\n",
    "        positions_df_mapped[[\"Player Name\", \"Position\"]], on=\"Player Name\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Check for missing positions after merge\n",
    "    missing_positions_count = merged_df[\"Position\"].isnull().sum()\n",
    "    if missing_positions_count > 0:\n",
    "        print(\n",
    "            f\"  âš ï¸ Warning: {missing_positions_count} players in '{output_filepath}' have missing 'Position' after merge.\"\n",
    "        )\n",
    "        print(\n",
    "            \"  This may indicate player name mismatches or unmapped positions in 'transfer_dataset.cleaned.csv'.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"  âœ… All players in '{output_filepath}' successfully matched with a 'Position'.\"\n",
    "        )\n",
    "\n",
    "    print(f\"  Merged DataFrame shape: {merged_df.shape}\")\n",
    "\n",
    "    # Step 4: Save the result to the current directory\n",
    "    merged_df.to_csv(output_filepath, index=False)\n",
    "    print(f\"Successfully saved processed data to: {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRpUZAQF-ChS"
   },
   "source": [
    "### 4.4 Process All Defined Datasets\n",
    "\n",
    "This loop iterates through the `datasets_to_process` list and calls the helper function for each, ensuring both pre and post-transfer datasets are enriched with position information and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_Nv_g5k-EBW",
    "outputId": "4c1e731e-528b-4ce1-c87f-dfcef32097fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: temp_stats.csv\n",
      "  Loaded stats data with shape: (148, 30)\n",
      "Merging with positions data...\n",
      "  âš ï¸ Warning: 35 players in '../1_datasets/cleaned/pre_transfer.cleaned.csv' have missing 'Position' after merge.\n",
      "  This may indicate player name mismatches or unmapped positions in 'transfer_dataset.cleaned.csv'.\n",
      "  Merged DataFrame shape: (148, 31)\n",
      "Successfully saved processed data to: ../1_datasets/cleaned/pre_transfer.cleaned.csv\n",
      "\n",
      "Processing: temp_stats.csv\n",
      "  Loaded stats data with shape: (148, 30)\n",
      "Merging with positions data...\n",
      "  âš ï¸ Warning: 35 players in '../1_datasets/cleaned/post_transfer.cleaned.csv' have missing 'Position' after merge.\n",
      "  This may indicate player name mismatches or unmapped positions in 'transfer_dataset.cleaned.csv'.\n",
      "  Merged DataFrame shape: (148, 31)\n",
      "Successfully saved processed data to: ../1_datasets/cleaned/post_transfer.cleaned.csv\n",
      "\n",
      "All specified datasets have been processed and saved with general position information.\n",
      "Data cleaning, merging, and position enrichment complete!\n",
      "\n",
      "Sample of pre-transfer and post-transfer datasets:\n",
      "\n",
      "Pre transfer cleaned dataset sample:\n",
      "      Player Name  Average Rating  Defending - Aerial duels won  \\\n",
      "0    Aaron Lennon            6.58                          0.20   \n",
      "1      Aaron Mooy            6.99                          0.77   \n",
      "2  Adam Armstrong            7.00                          0.32   \n",
      "3    Adam Webster            7.08                          6.28   \n",
      "4    Adama Traore            7.22                          1.12   \n",
      "\n",
      "   Defending - Blocked  Defending - Dribbled past  Defending - Duels won  \\\n",
      "0                 0.26                       0.74                   4.07   \n",
      "1                 0.49                       1.07                   4.56   \n",
      "2                 1.09                       0.39                   2.89   \n",
      "3                 0.31                       0.38                   9.30   \n",
      "4                 0.39                       0.57                  14.36   \n",
      "\n",
      "   Defending - Fouls committed  Defending - Interceptions  \\\n",
      "0                         1.42                       1.06   \n",
      "1                         0.88                       0.84   \n",
      "2                         0.93                       0.28   \n",
      "3                         0.82                       2.22   \n",
      "4                         1.40                       0.50   \n",
      "\n",
      "   Defending - Penalties conceded  Defending - Possession won final 3rd  ...  \\\n",
      "0                            0.02                                  0.52  ...   \n",
      "1                            0.00                                  0.84  ...   \n",
      "2                            0.00                                  0.66  ...   \n",
      "3                            0.01                                  0.06  ...   \n",
      "4                            0.02                                  0.55  ...   \n",
      "\n",
      "   Possession - Fouls won  Possession - Penalties awarded  \\\n",
      "0                    1.10                            0.00   \n",
      "1                    0.46                            0.00   \n",
      "2                    0.88                            0.00   \n",
      "3                    1.14                            0.00   \n",
      "4                    2.94                            0.02   \n",
      "\n",
      "   Possession - Successful dribbles  Possession - Touches  \\\n",
      "0                              0.90                 38.22   \n",
      "1                              0.66                 71.94   \n",
      "2                              1.15                 37.88   \n",
      "3                              0.62                 69.24   \n",
      "4                              8.86                 53.29   \n",
      "\n",
      "   Possession - Touches in opposition box  ShootingRank - Goals  \\\n",
      "0                                    1.66                  0.00   \n",
      "1                                    1.04                  0.12   \n",
      "2                                    5.18                  0.57   \n",
      "3                                    1.25                  0.04   \n",
      "4                                    3.12                  0.10   \n",
      "\n",
      "   ShootingRank - Shots  ShootingRank - Shots on target  Season_Year  Position  \n",
      "0                  0.30                            0.39       2019.5    Attack  \n",
      "1                  1.25                            0.38       2017.5  Midfield  \n",
      "2                  3.78                            1.72       2019.5    Attack  \n",
      "3                  0.45                            0.41       2017.5   Defense  \n",
      "4                  1.28                            0.36       2016.5    Attack  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Post transfer cleaned dataset sample:\n",
      "      Player Name  Average Rating  Defending - Aerial duels won  \\\n",
      "0    Aaron Lennon            6.58                          0.41   \n",
      "1      Aaron Mooy            6.86                          0.65   \n",
      "2  Adam Armstrong            6.66                          0.58   \n",
      "3    Adam Webster            6.94                          3.20   \n",
      "4    Adama Traore            7.10                          1.89   \n",
      "\n",
      "   Defending - Blocked  Defending - Dribbled past  Defending - Duels won  \\\n",
      "0                 0.23                       1.40                   4.24   \n",
      "1                 0.34                       1.46                   4.35   \n",
      "2                 0.64                       0.42                   2.82   \n",
      "3                 0.10                       0.53                   5.26   \n",
      "4                 0.51                       0.51                  12.34   \n",
      "\n",
      "   Defending - Fouls committed  Defending - Interceptions  \\\n",
      "0                         1.10                       0.52   \n",
      "1                         0.56                       0.99   \n",
      "2                         0.99                       0.26   \n",
      "3                         0.64                       1.58   \n",
      "4                         1.43                       0.53   \n",
      "\n",
      "   Defending - Penalties conceded  Defending - Possession won final 3rd  ...  \\\n",
      "0                            0.06                                  0.58  ...   \n",
      "1                            0.00                                  0.60  ...   \n",
      "2                            0.00                                  0.52  ...   \n",
      "3                            0.04                                  0.12  ...   \n",
      "4                            0.00                                  0.54  ...   \n",
      "\n",
      "   Possession - Fouls won  Possession - Penalties awarded  \\\n",
      "0                    1.16                            0.00   \n",
      "1                    0.47                            0.00   \n",
      "2                    0.50                            0.03   \n",
      "3                    0.58                            0.00   \n",
      "4                    2.64                            0.00   \n",
      "\n",
      "   Possession - Successful dribbles  Possession - Touches  \\\n",
      "0                              0.76                 34.94   \n",
      "1                              1.64                 63.07   \n",
      "2                              0.95                 33.63   \n",
      "3                              0.53                 76.42   \n",
      "4                              6.44                 60.16   \n",
      "\n",
      "   Possession - Touches in opposition box  ShootingRank - Goals  \\\n",
      "0                                    2.79                  0.12   \n",
      "1                                    1.85                  0.09   \n",
      "2                                    4.10                  0.13   \n",
      "3                                    1.10                  0.06   \n",
      "4                                    4.34                  0.12   \n",
      "\n",
      "   ShootingRank - Shots  ShootingRank - Shots on target  Season_Year  Position  \n",
      "0                  0.70                            0.23       2021.0    Attack  \n",
      "1                  1.42                            0.34       2019.0  Midfield  \n",
      "2                  2.34                            0.85       2021.5    Attack  \n",
      "3                  0.70                            0.24       2019.5   Defense  \n",
      "4                  1.70                            0.56       2018.5    Attack  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Process all datasets defined in the list using the helper function\n",
    "for dataset_info in datasets_to_process:\n",
    "    # Pass the DataFrame directly instead of a file path\n",
    "    stats_df = dataset_info[\"data\"]\n",
    "    output_filepath = dataset_info[\"output_file\"]\n",
    "\n",
    "    # Save the stats_df to a temporary CSV file to use with the helper function\n",
    "    temp_stats_path = \"temp_stats.csv\"\n",
    "    stats_df.to_csv(temp_stats_path, index=False)\n",
    "\n",
    "    # Use the helper function to process/merge and save\n",
    "    process_and_save_dataset(temp_stats_path, output_filepath, positions_df)\n",
    "\n",
    "    # Remove the temporary file after processing\n",
    "    os.remove(temp_stats_path)\n",
    "\n",
    "# Final message indicating completion of all processing steps\n",
    "print(\n",
    "    \"\\nAll specified datasets have been processed and saved with general position information.\"\n",
    ")\n",
    "print(\"Data cleaning, merging, and position enrichment complete!\")\n",
    "\n",
    "print(\"\\nSample of pre-transfer and post-transfer datasets:\")\n",
    "print(\"\\nPre transfer cleaned dataset sample:\")\n",
    "merged_pre_df = pd.read_csv(\"../1_datasets/cleaned/pre_transfer_cleaned.csv\")\n",
    "print(merged_pre_df.head())\n",
    "print(\"\\nPost transfer cleaned dataset sample:\")\n",
    "merged_post_df = pd.read_csv(\"../1_datasets/cleaned/post_transfer_cleaned.csv\")\n",
    "print(merged_post_df.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
